# ü•î Proyecto de An√°lisis y Gesti√≥n de Recaudo - Fedepapa

Este proyecto presenta una soluci√≥n integral para la gesti√≥n estad√≠stica del recaudo de la Cuota de Fomento de la Papa, integrando fuentes de producci√≥n, precios de mercado y registros de recaudo real.

---

## üèóÔ∏è 1. Modelamiento Inicial (Capa Conceptual)

En esta fase se realiz√≥ una arquitectura de datos preliminar sin aplicar transformaciones. El objetivo fue mapear la estructura cruda (*Raw Data*) para identificar las relaciones potenciales y las columnas cr√≠ticas para el negocio.

* **Estrategia:** Se defini√≥ un modelo que permitiera la trazabilidad desde la producci√≥n en campo hasta el recaudo en bancos.
* **Flexibilidad:** El dise√±o es evolutivo; se estructur√≥ para permitir ajustes en la l√≥gica de negocio (como el c√°lculo del 1% de fomento) sin comprometer la integridad de la data hist√≥rica.

> [!TIP]
> **Recursos de Dise√±o:**
> * üîó [Modelo Relacional Interactivo (dbdiagram.io)](https://dbdiagram.io/d/6999ced5bd82f5fce261dd12)
> * üñºÔ∏è [Visualizaci√≥n de la Ruta de Datos](https://github.com/EdithRP/Fedepapa_Analisis_y_Recaudo/blob/main/img/modeloinicial.png)

---

## ‚öôÔ∏è 2. Proceso de ETL (Python)

Se opt√≥ por **Python** para el desarrollo del pipeline de datos. Aunque el volumen actual permitir√≠a el uso de herramientas de hoja de c√°lculo, se prioriz√≥ la **escalabilidad, audibilidad y reproducibilidad**, aline√°ndome con los criterios de evaluaci√≥n de "Integraci√≥n y calidad del procesamiento".

### Estructura de Scripts (`/ETL`)
El flujo principal se gestiona desde el archivo `limpieza.py`, el cual coordina tres m√≥dulos especializados:

1.  **Carga y Limpieza T√©cnica:** Estandarizaci√≥n de encabezados y saneamiento de cadenas de texto.
2.  **Normalizaci√≥n de Fechas:** Unificaci√≥n de formatos cronol√≥gicos para permitir el cruce de datos semanales (precios) con mensuales (recaudo y producci√≥n).
3.  **Normalizaci√≥n Geogr√°fica:** Implementaci√≥n de la funci√≥n `departamentos()`, eliminando discrepancias de escritura para garantizar *Joins* precisos.

### Diagn√≥stico de Calidad (Data Profiling)
Durante el proceso se detect√≥ informaci√≥n faltante en las columnas `latitud` y `longitud` de la base de precios.
* **Criterio de Analista:** Se decidi√≥ **no imputar** estos datos. En esta etapa, el an√°lisis se centra en la agregaci√≥n departamental y no en la georreferenciaci√≥n puntual. Mantener la data real evita introducir sesgos artificiales en el modelo estad√≠stico.

---

## ‚òÅÔ∏è 3. Arquitectura en la Nube (BigQuery)

Para demostrar el m√°ximo potencial t√©cnico, la informaci√≥n procesada se carg√≥ en **Google BigQuery**. 

* **Capa de Staging (STG):** Los datos limpios de Python aterrizan en tablas de staging que preservan todas las columnas originales.
* **Dimensi√≥n de Tiempo Din√°mica:** Se implement√≥ una l√≥gica SQL que genera un calendario autom√°tico desde la fecha m√≠nima de recaudo hasta el `CURRENT_DATE`, asegurando que el modelo sea autosuficiente para futuros reportes.
* **Capa Sem√°ntica (Views):** Los c√°lculos de brechas y KPIs se realizan mediante vistas SQL, entregando a **Power BI** datos ya transformados para un rendimiento √≥ptimo.
