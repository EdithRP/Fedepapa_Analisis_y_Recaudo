# ü•î Proyecto de An√°lisis y Gesti√≥n de Recaudo - Fedepapa

Este proyecto presenta una soluci√≥n integral para la gesti√≥n estad√≠stica del recaudo de la Cuota de Fomento de la Papa, integrando fuentes de producci√≥n, precios de mercado y registros de recaudo real.

---

## üèóÔ∏è 1. Modelamiento Inicial (Capa Conceptual)

En esta fase se realiz√≥ una arquitectura de datos preliminar sin aplicar transformaciones. El objetivo fue mapear la estructura cruda (*Raw Data*) para identificar las relaciones potenciales y las columnas cr√≠ticas para el negocio.

* **Estrategia:** Se defini√≥ un modelo que permitiera la trazabilidad desde la producci√≥n en campo hasta el recaudo en bancos.
* **Flexibilidad:** El dise√±o es evolutivo; se estructur√≥ para permitir ajustes en la l√≥gica de negocio (como el c√°lculo del 1% de fomento) sin comprometer la integridad de la data hist√≥rica.

> [!TIP]
> **Recursos de Dise√±o:**
> * üîó [Modelo Relacional Interactivo (dbdiagram.io)](https://dbdiagram.io/d/6999ced5bd82f5fce261dd12)
> * üñºÔ∏è [Visualizaci√≥n de la Ruta de Datos](https://github.com/EdithRP/Fedepapa_Analisis_y_Recaudo/blob/main/img/modeloinicial.png)

---

## ‚öôÔ∏è 2. Proceso de ETL (Python)

Se opt√≥ por **Python** para el desarrollo del pipeline de datos. Aunque el volumen actual permitir√≠a el uso de herramientas de hoja de c√°lculo, se prioriz√≥ la **escalabilidad y reproducibilidad**, aline√°ndome con los criterios de evaluaci√≥n de "Integraci√≥n y calidad del procesamiento".

### Estructura de Scripts ¬®[(`/ETL`)](https://github.com/EdithRP/Fedepapa_Analisis_y_Recaudo/tree/main/ETL)
El flujo principal se gestiona desde el archivo ¬®[`limpieza.py`](https://github.com/EdithRP/Fedepapa_Analisis_y_Recaudo/blob/main/ETL/limpieza.ipynb), el cual coordina tres m√≥dulos especializados:

1.  **Carga y Limpieza T√©cnica:** Estandarizaci√≥n de encabezados y carga de archivos.
2.  **Normalizaci√≥n de Fechas:** Unificaci√≥n de formatos cronol√≥gicos para permitir el cruce de datos semanales (precios) con mensuales (recaudo y producci√≥n).
3.  **Normalizaci√≥n Geogr√°fica:** Implementaci√≥n de la funci√≥n [`normalizar_departamentos()`](https://github.com/EdithRP/Fedepapa_Analisis_y_Recaudo/blob/main/ETL/normalizacion_departamento.ipynb), eliminando discrepancias de escritura para garantizar *Joins* precisos.

### Diagn√≥stico de Calidad (Data Profiling)
Durante el proceso se detect√≥ informaci√≥n faltante en las columnas `latitud` y `longitud` de la base de precios.
* **Criterio de Analisis:** Se decidi√≥ **no imputar** estos datos. En esta etapa, el an√°lisis se centra en la agregaci√≥n departamental y no en la georreferenciaci√≥n puntual. Mantener la data real evita introducir sesgos artificiales en el modelo estad√≠stico.

---
## üèóÔ∏è 3. Arquitectura de Datos y Modelado en BigQuery

Para garantizar la escalabilidad, se implement√≥ una arquitectura de capas (Staging y Warehouse) en la regi√≥n `southamerica-west1`.

### Paso 3.1: Ingesta Autom√°tica a BigQuery
Se desarroll√≥ un pipeline de carga utilizando la librer√≠a `pandas-gbq` y el SDK de Google Cloud. Este proceso garantiza que los datos limpios de la fase anterior se alojen de forma segura en el dataset de Staging.

* **Script de Carga:** [Ver Script de Carga en Python](https://github.com/EdithRP/Fedepapa_Analisis_y_Recaudo/blob/main/cargar_bigquery.py) 
* **Dataset de Destino:** `sgt_fedepapa`.

### Paso 3.2: Creaci√≥n de Tablas de Dimensi√≥n
Para eliminar la redundancia y permitir un an√°lisis temporal y geogr√°fico preciso, se crearon tablas maestras mediante SQL:

* **Dimensi√≥n Tiempo (`dim_tiempo`)**: Centraliza la jerarqu√≠a de A√±o, Mes (en espa√±ol), Semestre y N√∫mero de Semana. Es vital para unir la producci√≥n mensual con los precios semanales.
* **Dimensi√≥n Geograf√≠a (`dim_geografia`)**: Unifica los nombres de los departamentos bajo el campo `departamento_normalizado`, resolviendo discrepancias de escritura entre las fuentes originales.


### Paso 3.3: Creaci√≥n de Tablas de Hechos (M√©tricas)
Se generaron tablas de hechos (`fct_`) normalizadas que contienen exclusivamente las m√©tricas necesarias para las actividades de la prueba, eliminando columnas de texto redundantes:

* **`fct_precios`**: Registro hist√≥rico de precios por variedad y ciudad.
* **`fct_produccion`**: Cifras de producci√≥n donde se aplic√≥ la l√≥gica de conversi√≥n de Toneladas a Kilogramos.
* **`fct_recaudo`**: Detalle financiero del recaudo real e intereses de mora.

### üí° Resumen del Proceso de Ingenier√≠a
Se opt√≥ por este proceso de **Modelado Dimensional (Star Schema)** en BigQuery por tres razones fundamentales:

1.  **Optimizaci√≥n en Power BI**: Los modelos en estrella son significativamente m√°s r√°pidos y eficientes para el motor DAX, evitando relaciones de "muchos a muchos".
2.  **Integridad de Datos**: Al centralizar los nombres de meses y departamentos en tablas √∫nicas, se garantiza que no existan inconsistencias al filtrar la informaci√≥n.
3.  **Preparaci√≥n para el An√°lisis**: Al realizar la conversi√≥n de unidades (Ton a Kg) y la limpieza geogr√°fica en el Warehouse (SQL), se entrega un dato "listo para consumir", cumpliendo con los est√°ndares de un entorno profesional de Ingenier√≠a de Datos.

---

## üöÄ 4. Actividades de la Prueba
1. **Actividad 1**: Estimaci√≥n mensual del recaudo potencial integrando producci√≥n y precios.
   
3. **Actividad 2**: An√°lisis de brechas (Potencial vs Real) y volatilidad.
4. **Actividad 3**: C√°lculo de concentraci√≥n (HHI) y segmentaci√≥n de recaudadores.
